{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcb2b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lorem\n",
      "  Downloading lorem-0.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading lorem-0.1.1-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: lorem\n",
      "Successfully installed lorem-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1b1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lorem\n",
    "import json\n",
    "\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for i in range(3):\n",
    "        paragraph = lorem.paragraph()\n",
    "        file.write(paragraph)\n",
    "        file.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f6005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2 :  Update the txt file by removing the first paragraph.\n",
    "\n",
    "with open('lorem_ipsum.txt', 'r') as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "paragraphs.pop(0)\n",
    "\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for paragraph in paragraphs:\n",
    "        file.write(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50e1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 : Create a dict from the paper of [lecun et al.](https://www.researchgate.net/publication/277411157_Deep_Learning) and [goodfellow et al.](https://arxiv.org/abs/1406.2661) with authors, title, affiliations.\n",
    "\n",
    "papers = {\n",
    "    \"lecun\": {\n",
    "        \"authors\": [\"Yann LeCun\", \"Yoshua Bengio\", \"Geoffrey Hinton\"],\n",
    "        \"title\": \"Deep Learning\",\n",
    "        \"affiliations\": [\"New York University\", \"University of Montreal\", \"University of Toronto\"]\n",
    "    },\n",
    "    \"goodfellow\": {\n",
    "        \"authors\": [\"Ian J. Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "        \"title\": \"Deep Learning\",\n",
    "        \"affiliations\": [\"Google\", \"University of Montreal\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12efd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lecun': {'authors': ['Yann LeCun', 'Yoshua Bengio', 'Geoffrey Hinton'], 'title': 'Deep Learning', 'affiliations': ['New York University', 'University of Montreal', 'University of Toronto']}, 'goodfellow': {'authors': ['Ian J. Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'title': 'Deep Learning', 'affiliations': ['Google', 'University of Montreal']}}\n"
     ]
    }
   ],
   "source": [
    "#4 : Save the previously created dict in the JSON format and load it back.\n",
    "\n",
    "with open('papers.json', 'w') as file:\n",
    "    json.dump(papers, file)\n",
    "with open('papers.json', 'r') as file:\n",
    "    loaded_papers = json.load(file)\n",
    "print(loaded_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513eee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\x14\\x01\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x05lecun\\x94}\\x94(\\x8c\\x07authors\\x94]\\x94(\\x8c\\nYann LeCun\\x94\\x8c\\rYoshua Bengio\\x94\\x8c\\x0fGeoffrey Hinton\\x94e\\x8c\\x05title\\x94\\x8c\\rDeep Learning\\x94\\x8c\\x0caffiliations\\x94]\\x94(\\x8c\\x13New York University\\x94\\x8c\\x16University of Montreal\\x94\\x8c\\x15University of Toronto\\x94eu\\x8c\\ngoodfellow\\x94}\\x94(h\\x03]\\x94(\\x8c\\x11Ian J. Goodfellow\\x94h\\x06\\x8c\\x0fAaron Courville\\x94eh\\x08h\\th\\n]\\x94(\\x8c\\x06Google\\x94h\\reuu.'\n"
     ]
    }
   ],
   "source": [
    "#5 : Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n",
    "\n",
    "import pickle\n",
    "with open('papers.pickle', 'wb') as file:\n",
    "    pickle.dump(papers, file)\n",
    "with open('papers.pickle', 'rb') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5a6b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#6 : Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "tree = ET.parse(\"C:\\\\Users\\\\carlf\\\\OneDrive\\\\Bureau\\\\NoSQL_SQL\\\\xml_file2.nxml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "data = {}\n",
    "for child in root:\n",
    "    if child.tag == 'name':\n",
    "        data['name'] = child.text\n",
    "    elif child.tag == 'age':\n",
    "        data['age'] = int(child.text)\n",
    "    elif child.tag == 'email':\n",
    "        data['email'] = child.text\n",
    "    elif child.tag == 'phone':\n",
    "        data['phone'] = child.text\n",
    "\n",
    "with open('xml_data.json', 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "with open('xml_data.json', 'r') as file:\n",
    "    contents = json.load(file)\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b06710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Download an image of your choice and save it in either jpg or png.\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(requests.get(\"https://img.freepik.com/vecteurs-libre/eclaboussure-realiste-jus-huile-png-transparent_107791-18291.jpg\", stream=True).raw)\n",
    "im.save(\"C:\\\\Users\\\\carlf\\\\OneDrive\\\\Bureau\\\\NoSQL_SQL\\\\Guts\", \"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4723022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# 8 : From the data/Chap2/data_world.json file, create a set of publisher type.\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"C:\\\\Users\\\\carlf\\\\OneDrive\\\\Bureau\\\\NoSQL_SQL\\\\data_world.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "publisher_types = set()\n",
    "for book in data:\n",
    "    if 'publisher_type' in book:\n",
    "        publisher_types.add(book['publisher_type'])\n",
    "\n",
    "print(publisher_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f82734d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 : From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"C:\\\\Users\\\\carlf\\\\OneDrive\\\\Bureau\\\\NoSQL_SQL\\\\data_world.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for book in data:\n",
    "    if 'subtitle' in book:\n",
    "        del book['subtitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72081e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public | irregular | 4961\n",
      "public | R/P1D | 5\n",
      "public | R/P1M | 3\n",
      "public | Unknown | 29\n",
      "public | R/PT1S | 1\n",
      "public | R/P3M | 1\n"
     ]
    }
   ],
   "source": [
    "#10 : From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\".\n",
    "\n",
    "matrix = {}\n",
    "\n",
    "with open(\"C:\\\\Users\\\\carlf\\\\OneDrive\\\\Bureau\\\\NoSQL_SQL\\\\data_world.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for book in data:\n",
    "    access_level = book['accessLevel']\n",
    "    if 'accrualPeriodicity' in book:\n",
    "        accrual_periodicity = book['accrualPeriodicity']\n",
    "    else:\n",
    "        accrual_periodicity = 'Unknown'\n",
    "    if access_level not in matrix:\n",
    "        matrix[access_level] = {}\n",
    "    if accrual_periodicity not in matrix[access_level]:\n",
    "        matrix[access_level][accrual_periodicity] = 0\n",
    "    matrix[access_level][accrual_periodicity] += 1\n",
    "\n",
    "for access_level in matrix:\n",
    "    for accrual_periodicity in matrix[access_level]:\n",
    "        count = matrix[access_level][accrual_periodicity]\n",
    "        print(f'{access_level} | {accrual_periodicity} | {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a644d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
